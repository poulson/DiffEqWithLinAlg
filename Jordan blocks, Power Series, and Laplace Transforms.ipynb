{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math 53: Differential Equations with Linear Algebra, Prof. Jack Poulson\n",
    "### Jordan Blocks, Power Series, and Laplace Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial conditions as combinations of eigenvectors\n",
    "Consider the vector Initial Value Problem\n",
    "$$\n",
    "\\dot z(t) = A z(t),\\;\\;\\;z(0)=q,\n",
    "$$\n",
    "in the case where $A q = \\lambda q$; that is, the system is initialized with the eigenvector $q$ (which has eigenvalue $\\lambda$). Since the solution is \n",
    "$$\n",
    "z(t) = \\exp(t A) q = \\left(\\sum_{j=0}^\\infty \\frac{(tA)^k}{k!} \\right) q,\n",
    "$$\n",
    "and $A q = \\lambda q$ implies that $A^2 q = A (A q) = A (\\lambda q) = \\lambda (A q) = \\lambda^2 q$, and, more generally, that $A^k q = \\lambda^k q$ and $p(A) q = p(\\lambda) q$ for any polynomial $p$. Thus,\n",
    "$$\n",
    "z(t) = \\exp(t A) q = \\exp(t \\lambda) q,\n",
    "$$\n",
    "which shows that, if $z$ is initialized in the direction of an eigenvector, it stays in that direction and its magnitude changes as $\\exp(t \\lambda)$.\n",
    "\n",
    "Now suppose that the initial condition is the sum of two eigenvectors, say,\n",
    "$$\n",
    "z(0) = \\alpha q_1 + \\beta q_2,\n",
    "$$\n",
    "where $A q_1 = \\lambda_1 q_1$ and $A q_2 = \\lambda_2 q_2$. Then\n",
    "$$\n",
    "z(t) = \\exp(t A) (\\alpha q_1 + \\beta q_2) = \\alpha \\exp(t A) q_1 + \\beta \\exp(t A) q_2 = \\alpha \\exp(t\\lambda_1) q_1 + \\beta \\exp(t\\lambda_2) q_2,\n",
    "$$\n",
    "which shows that the simple manner in which the amplitude of each component of the initial condition evolves over time. It is thus natural to ask the question of whether it is *always* possible to decompose an initial state into a sum of eigenvectors. Unfortunately, the answer is \"no\", as doing so would require that we can always solve the problem\n",
    "$$\n",
    "  X c = \\sum_{j=1}^n c_j x_j = z(0)\n",
    "$$\n",
    "for the vector of eigenvector coefficients $c$. But this would generally involve setting \n",
    "$$\n",
    "  c = X^{-1} z(0),\n",
    "$$\n",
    "which would require that the eigenvectors of $A$ form a complete basis (so that $X$ is not singular). And we have repeatedly discussed the matrix\n",
    "$$\n",
    "A = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix},\n",
    "$$\n",
    "which only has a single eigenvector, so it is not generally possible to express every initial condition as a sum of eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized eigenvectors of a defective two-by-two matrix\n",
    "Consider a two-by-two matrix $A$ which has a repeated eigenvalue $\\lambda$ but only a single eigenvector, say $A x_1 = \\lambda x_1$. Since $A$ has a repeated eigenvalue, the Schur decomposition of $A$,\n",
    "$$ A = Q T Q^H, $$\n",
    "where $Q$ is unitary ($Q^H Q = Q Q^H = I$) and $T$ is upper-triangular with the eigenvalues of $A$ along its diagonal, must be of the form\n",
    "$$ A = Q \\begin{pmatrix} \\lambda & \\gamma \\\\ 0 & \\lambda \\end{pmatrix} Q^H, $$\n",
    "for some value of $\\gamma$. And since any vector in the null-space of $(A-\\lambda I)$ must be proportional to $x_1$, \n",
    "$$\n",
    "(A-\\lambda I)^2 = \\left(Q \\begin{pmatrix} 0 & \\gamma \\\\ 0 & 0 \\end{pmatrix} Q^H\\right)^2 = Q \\begin{pmatrix} 0 & \\gamma \\\\ 0 & 0 \\end{pmatrix}^2 Q^H = Q \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} Q^H = 0\n",
    "$$\n",
    "shows that *every* vector is in the null-space of $(A-\\lambda I)^2$. Thus, there exists a vector which is in the null space of $(A-\\lambda I)^2$ but *not* in the null-space of $(A-\\lambda I)$ (in fact, every vector which is not parallel to $x_1$ satisfies this property in our two-by-two example). Such a vector is called a **generalized eigenvector**; more generally, for any matrix $A$, a vector $x$ is said to be a **generalized eigenvector of rank $m$** if \n",
    "$$\n",
    "(A - \\lambda I)^m x = 0,\\;\\;\\; (A-\\lambda I)^{m-1} x \\neq 0.\n",
    "$$\n",
    "\n",
    "For our two-by-two example, given a generalized eigenvector of rank 2,\n",
    "$$\n",
    "(A - \\lambda I)^2 x_2 = 0,\\;\\;\\; (A-\\lambda I) x \\neq 0,\n",
    "$$\n",
    "it is enlightening to expand the left expression into the form\n",
    "$$\n",
    "(A - \\lambda I)\\left[(A-\\lambda I) x_2\\right] = 0,\n",
    "$$\n",
    "which shows that the vector $(A-\\lambda I) x_2$ is in the null-space of $(A-\\lambda I)$ and must therefore be proportional to the only proper eigenvector, $x_1$. Choosing $x_2$ so that this constant of proportionality is one, we find\n",
    "$$\n",
    "(A - \\lambda I) x_2 = x_1,\n",
    "$$\n",
    "which can be re-arranged into the form\n",
    "$$\n",
    "A x_2 = x_1 + \\lambda x_2 = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\lambda \\end{pmatrix}.\n",
    "$$\n",
    "Since we already know that \n",
    "$$\n",
    "A x_1 = \\lambda x_1 = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\begin{pmatrix} \\lambda \\\\ 0 \\end{pmatrix},\n",
    "$$\n",
    "we can combine these two statements into the matrix equation\n",
    "$$\n",
    "A \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix}.\n",
    "$$\n",
    "And since the generalized eigenvector $x_2$ must be linearly independent of $x_1$ by definition (otherwise $(A - \\lambda I) x_2$ would be zero), we can apply the inverse of $X=\\begin{pmatrix} x_1 & x_2 \\end{pmatrix}$ from the right to each side of the above equation to find\n",
    "$$\n",
    "A = X \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix} X^{-1},\n",
    "$$\n",
    "which an example of a **Jordan Canonical Form**, which expresses a matrix in terms of its (generalized) eigenvectors in a manner which is similar to a spectral decomposition, but with ones allowed in particular places just above the diagonal of eigenvalues. Before we give a precise definition of Jordan Canonical Form, it is helpful to first discuss **Jordan blocks** and their algebraic properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomials of Jordan blocks\n",
    "A **Jordan block** of dimension $n$ with eigenvalue $\\lambda$, $J_{\\lambda}(n)$, is defined as the $n \\times n$ matrix\n",
    "$$\n",
    "J_{\\lambda}(n) = \\begin{pmatrix} \\lambda & 1 & 0 & \\cdots & 0 & 0 \\\\ 0 & \\lambda & 1 & \\cdots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & \\lambda & 1 \\\\ 0 & 0 & 0 & \\vdots & 0 & \\lambda \\end{pmatrix}.\n",
    "$$\n",
    "For example,\n",
    "\n",
    "\\begin{align}\n",
    "J_{\\lambda}(1) &= \\lambda, \\\\\n",
    "J_{\\lambda}(2) &= \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix},\\text{ and}\\\\\n",
    "J_{\\lambda}(3) &= \\begin{pmatrix} \\lambda & 1 & 0 \\\\ 0 & \\lambda & 1 \\\\ 0 & 0 & \\lambda \\end{pmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "While Jordan blocks are not as easy to take functions of as diagonal matrices, we will see that they are *almost* as convenient. For example,\n",
    "$$\n",
    "\\left(J_{\\lambda}(2)\\right)^2 = \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix}^2 = \\begin{pmatrix} \\lambda^2 & 2 \\lambda \\\\ 0 & \\lambda^2 \\end{pmatrix},\n",
    "$$\n",
    "$$\n",
    "\\left(J_{\\lambda}(2)\\right)^3 = \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix}^3 = \\begin{pmatrix} \\lambda^3 & 3 \\lambda^2 \\\\ 0 & \\lambda^3 \\end{pmatrix},\n",
    "$$\n",
    "and, more generally, we can verify (for example, through induction) that\n",
    "$$\n",
    "\\left(J_{\\lambda}(2)\\right)^k = \\begin{pmatrix} \\lambda^k & k \\lambda^{k-1} \\\\ 0 & \\lambda^k \\end{pmatrix}\n",
    "$$\n",
    "for any positive integer $k$. The top-right entry is conveniently the derivative of $\\lambda^k$ with respect to $\\lambda$, and so we would therefore find that\n",
    "$$\n",
    "p(J_{\\lambda}(2)) = \\begin{pmatrix} p(\\lambda) & p'(\\lambda) \\\\ 0 & p(\\lambda) \\end{pmatrix}\n",
    "$$\n",
    "for any polynomial $p$. And, by extension, we would have that\n",
    "$$\n",
    "\\exp(t J_{\\lambda}(2)) = \\begin{pmatrix} e^{\\lambda t} & t e^{\\lambda t} \\\\ 0 & e^{\\lambda t}\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Further, it can be verified that\n",
    "$$\n",
    "p(J_{\\lambda}(3)) = \\begin{pmatrix} p(\\lambda) & p'(\\lambda) & \\frac{p''(\\lambda)}{2} \\\\ 0 & p(\\lambda) & p'(\\lambda) \\\\ 0 & 0 & p(\\lambda)\\end{pmatrix}\n",
    "$$\n",
    "and, in the general case, that\n",
    "$$\n",
    "p(J_{\\lambda}(n)) = \\begin{pmatrix}\n",
    "p(\\lambda) & p'(\\lambda) & \\frac{p''(\\lambda)}{2!} & \\cdots & \\frac{p^{(n-2)}(\\lambda)}{(n-2)!} & \\frac{p^{(n-1)}(\\lambda)}{(n-1)!} \\\\\n",
    "0 & p(\\lambda) & p'(\\lambda) & \\cdots & \\frac{p^{(n-3)}(\\lambda)}{(n-3)!} & \\frac{p^{(n-2)}(\\lambda)}{(n-2)!} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & p(\\lambda) & p'(\\lambda) \\\\\n",
    "0 & 0 & 0 & \\cdots & 0 & p(\\lambda)\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "Thus, it is relatively straightforward to form polynomials (and exponentials) of Jordan blocks of arbitrary size. In the important case of the matrix exponential, we have that\n",
    "$$\n",
    "\\exp(t J_{\\lambda}(n)) = \\exp(t\\lambda) \\begin{pmatrix}\n",
    "1 & t & \\frac{t^2}{2!} & \\cdots & \\frac{t^{n-2}}{(n-2)!} & \\frac{t^{n-1}}{(n-1)!} \\\\\n",
    "0 & 1 & t & \\cdots & \\frac{t^{n-3}}{(n-3)!} & \\frac{t^{n-2}}{(n-2)!} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & 1 & t \\\\\n",
    "0 & 0 & 0 & \\cdots & 0 & 1\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jordan Canonical Form\n",
    "One of the most important theoretical (but impractical for real-world calculations due to ambiguity as to whether eigenvalues are unique or just very close) tools in linear algebra is a relaxation of an eigenvalue decomposition known as **Jordan canonical form**:\n",
    "$$\n",
    "  A = X J X^{-1} = \\begin{pmatrix} X_1 & X_2 & \\cdots & X_m \\end{pmatrix} \\begin{pmatrix} J_{\\lambda_1}(n_1) & 0 & \\cdots & 0 \\\\ 0 & J_{\\lambda_2}(n_2) & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & J_{\\lambda_m}(n_m) \\end{pmatrix} \\begin{pmatrix} X_1 & X_2 & \\cdots X_m \\end{pmatrix}^{-1},\n",
    "$$\n",
    "where $X_k$ is the matrix of (generalized) eigenvectors of $A$ associated with the eigenvalue $\\lambda_k$ such that\n",
    "$$\n",
    "  A X_k = X_k J_{\\lambda_k}(n_k).\n",
    "$$\n",
    "In particular, when every repeated eigenvalue has a full set of eigenvectors (for example, consider the identity matrix), the Jordan canonical form reduces to an eigenvalue decomposition.\n",
    "\n",
    "Forming polynomials of matrices becomes straight-forward given the existence of a Jordan canonical form, as\n",
    "$$\n",
    "  A^2 = (X J X^{-1}) (X J X^{-1}) = X J^2 X^{-1} = X \\begin{pmatrix} (J_{\\lambda_1}(n_1))^2 & 0 & \\cdots & 0 \\\\ 0 & (J_{\\lambda_2}(n_2))^2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & (J_{\\lambda_m}(n_m))^2 \\end{pmatrix} X^{-1},\n",
    "$$\n",
    "and, more generally, for any polynomial $p$,\n",
    "$$\n",
    "  p(A) = X p(J) X^{-1} = X \\begin{pmatrix} p(J_{\\lambda_1}(n_1)) & 0 & \\cdots & 0 \\\\ 0 & p(J_{\\lambda_2}(n_2)) & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & p(J_{\\lambda_m}(n_m)) \\end{pmatrix} X^{-1}.\n",
    "$$\n",
    "And since it is straight-forward to compute polynomials of Jordan blocks, it is similarly simple to form a polynomial of a matrix whose Jordan canonical form is available.\n",
    "\n",
    "##### Example: Taking polynomials and exponentials using Jordan canonical form\n",
    "Consider the matrix\n",
    "$$\n",
    "A = X \\begin{pmatrix}\n",
    "  J_2(1) & 0 & 0 & 0 \\\\\n",
    "  0 & J_2(2) & 0 & 0 \\\\\n",
    "  0 & 0 & J_i(3) & 0 \\\\\n",
    "  0 & 0 & 0 & J_0(1) \\\\\n",
    "  \\end{pmatrix} X^{-1} =\n",
    "X \\left(\\begin{array}{c|cc|ccc|c}\n",
    "  2 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "  \\hline\n",
    "  0 & 2 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "  0 & 0 & 2 & 0 & 0 & 0 & 0 \\\\\n",
    "  \\hline\n",
    "  0 & 0 & 0 & i & 1 & 0 & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & i & 1 & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & i & 0 \\\\\n",
    "  \\hline\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "  \\end{array}\\right) X^{-1},\n",
    "$$\n",
    "where $X$ is an arbitrary (invertible) matrix. Then\n",
    "$$\n",
    "A^2 = X \\begin{pmatrix}\n",
    "  (J_2(1))^2 & 0 & 0 & 0 \\\\\n",
    "  0 & (J_2(2))^2 & 0 & 0 \\\\\n",
    "  0 & 0 & (J_i(3))^2 & 0 \\\\\n",
    "  0 & 0 & 0 & (J_0(1))^2\n",
    "  \\end{pmatrix} X^{-1} =\n",
    "X \\left(\\begin{array}{c|cc|ccc}\n",
    "  4 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "  \\hline\n",
    "  0 & 4 & 4 & 0 & 0 & 0 & 0 \\\\\n",
    "  0 & 0 & 4 & 0 & 0 & 0 & 0 \\\\\n",
    "  \\hline\n",
    "  0 & 0 & 0 & -1 & 2i & 0 & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & -1 & 2i & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & -1 & 0 \\\\\n",
    "  \\hline\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "  \\end{array}\\right) X^{-1},\n",
    "$$\n",
    "and, likewise,\n",
    "$$\n",
    "\\exp(tA) = X \\begin{pmatrix}\n",
    "  \\exp(t J_2(1)) & 0 & 0 & 0 \\\\\n",
    "   0 & \\exp(t J_2(2)) & 0 & 0 \\\\\n",
    "   0 & 0 & \\exp(t J_i(3)) & 0 \\\\\n",
    "   0 & 0 & 0 & \\exp(t J_0(1))\n",
    " \\end{pmatrix} X^{-1} =\n",
    "X \\left(\\begin{array}{c|cc|ccc|c}\n",
    "  e^{2t} & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "  \\hline\n",
    "  0 & e^{2t} & t e^{2t} & 0 & 0 & 0 & 0 \\\\\n",
    "  0 & 0 & e^{2t} & 0 & 0 & 0 & 0 \\\\\n",
    "  \\hline\n",
    "  0 & 0 & 0 & e^{it} & t e^{it} & 0 & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & e^{it} & t e^{it} & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & e^{it} & 0 \\\\\n",
    "  \\hline\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 1\n",
    "  \\end{array}\\right) X^{-1}.\n",
    "$$\n",
    "This particular matrix was chosen to emphasize the fact that different Jordan blocks can have the same eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power series solutions\n",
    "We have now covered various techniques for solving (systems of) differential equations without forcing functions, but we have do not yet have a toolkit for solving second-order differential equations with nonzero \"forcing functions\", e.g.,\n",
    "$$\n",
    "y''(x) + \\lambda^2 y(x) = g(x),\n",
    "$$\n",
    "where $g(x)$ is not equal to zero. Furthermore, we also have not discussed techniques for solving **nonlinear** second-order differential equations, such as\n",
    "$$\n",
    "y''(x) - x y(x) = 0,\n",
    "$$\n",
    "which is known as **Airy's equation**. One of the most popular techniques for exploring solutions to such equations is the **method of power series**, which assumes that $y(x)$ has a power series representation, say\n",
    "$$\n",
    "y(x) = \\sum_{k=0}^\\infty \\alpha_k x^k,\n",
    "$$\n",
    "and then deduces how the coefficients must be related as a consequence. In the case of forced differential equations, it is crucial that the forcing function, say $g(x)$, have a power series representation (for example, this requires that $g(x)$ is discontinuous; the method of **Laplace transforms** is discussed below and can handle discontinuous forcing functions).\n",
    "\n",
    "##### A return to the mass-spring system\n",
    "**NOTE: This section of notes will soon be filled in.**\n",
    "\n",
    "##### The Airy equation\n",
    "$$\n",
    "y''(x) - x y(x) = 0\n",
    "$$\n",
    "**NOTE: This section of notes will soon be filled in.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Transforms\n",
    "While our previous discussion of power series analysis was useful for solving differential equations with \"forcing functions\" which have power series representations, it does not provide a way for one to analyze systems whose forcing functions have discontinuities.\n",
    "\n",
    "Consider the following identity implied by Integration by Parts:\n",
    "$$\n",
    "\\int_0^\\infty e^{-st} y'(t) dt = \\left[ e^{-st} y(t)\\right]_0^\\infty + s \\int_0^\\infty e^{-st} y(t) dt.\n",
    "$$\n",
    "Under the assumption that $s$ is a sufficiently positive real number, $e^{-st}$ should decay rapidly enough to overtake any growth in $y(t)$ so that $e^{-st} y(t)$ also tends to zero at $t$ goes to infinity. Then, for such a large value of $s$,\n",
    "$$\n",
    "\\int_0^\\infty e^{-st} y'(t) dt = -y(0) + s \\int_0^\\infty e^{-st} y(t) dt.\n",
    "$$\n",
    "Defining the **Laplace transform** of a function $g(t)$ evaluated at $s$ as\n",
    "$$\n",
    "\\mathcal{L}[g](s) = \\int_0^\\infty e^{-st} g(t) dt,\n",
    "$$\n",
    "our previous result showed that\n",
    "$$\n",
    "\\mathcal{L}[y'](s) = s \\mathcal{L}[y](s) - y(0),\n",
    "$$\n",
    "which provides a convenient relationship between the Laplace transforms of $y$ and $y'$ (again, assuming $s$ is sufficiently large). Similarly, applying integration by parts twice, and assuming that $s$ is sufficiently large, we can deduce that\n",
    "$$\n",
    "\\mathcal{L}[y''](s) = \\int_0^\\infty e^{-st} y''(t) dt = s^2 \\mathcal{L}[y](s) - y'(0) - s y(0).\n",
    "$$\n",
    "The Laplace transform of exponentials is similarly convenient:\n",
    "$$\n",
    "\\mathcal{L}[e^{-\\alpha t}](s) = \\int_0^\\infty e^{-st} e^{-\\alpha t} dt = \\int_0^\\infty e^{-(s+\\alpha) t} dt = \\frac{1}{s+\\alpha},\n",
    "$$\n",
    "with the last equality using the assumption that $s+\\alpha$ is positive.\n",
    "\n",
    "##### Example: Solving the unforced mass-spring system via Laplace transforms\n",
    "One can now solve the differential equation\n",
    "$$\n",
    "y''(t) + \\lambda^2 y(t) = 0\n",
    "$$\n",
    "via Laplace transforms. First, we apply the Laplace transform to both sides to conclude that\n",
    "$$\n",
    "(s^2 \\mathcal{L}[y](s) - y'(0) - s y(0)) + \\lambda^2 \\mathcal{L}[y](s) = 0,\n",
    "$$\n",
    "and thus\n",
    "$$\n",
    "\\mathcal{L}[y](s) = \\frac{y'(0) + s y(0)}{s^2 + \\lambda^2} = \\frac{y'(0) + s y(0)}{(s+i\\lambda)(s-i\\lambda)}.\n",
    "$$\n",
    "The main computational hurdle is now to use a **Partial Fraction Expansion** to find the $P$ and $Q$ such that\n",
    "$$\n",
    "\\frac{y'(0) + s y(0)}{(s+i\\lambda)(s-i\\lambda)} = \\frac{P}{s+i\\lambda} + \\frac{Q}{s-i\\lambda} = \\frac{P(s-i\\lambda)+Q(s+i\\lambda)}{(s+i\\lambda)(s-i\\lambda)} = \\frac{(P+Q)s + i \\lambda (Q-P) s}{(s+i\\lambda)(s-i\\lambda)}.\n",
    "$$\n",
    "For a reason related to our previous power series analysis, we can equate the coefficients on $s^0$ and $s^1$ to find that\n",
    "\\begin{align}\n",
    "  P+Q &= y(0),\\;\\text{and} \\\\\n",
    "  Q-P &= \\frac{y'(0)}{i\\lambda}.\n",
    "\\end{align}\n",
    "Then, summing the two equations implies that\n",
    "$$\n",
    "Q = \\frac{1}{2}\\left(y(0)+\\frac{y'(0)}{i\\lambda}\\right),\n",
    "$$\n",
    "which then implies that\n",
    "$$\n",
    "P = \\frac{1}{2}\\left(y(0)-\\frac{y'(0)}{i\\lambda}\\right).\n",
    "$$\n",
    "Furthermore, since\n",
    "$$\n",
    "\\mathcal{L}[y](s) = P \\frac{1}{s+i\\lambda} + Q \\frac{1}{s-i\\lambda},\n",
    "$$\n",
    "apply the inverse Laplace transform to both sides implies\n",
    "$$\n",
    "y(t) = P \\mathcal{L}^{-1}\\left[\\frac{1}{s+i\\lambda}\\right] + Q \\mathcal{L}^{-1}\\left[\\frac{1}{s-i\\lambda}\\right],\n",
    "$$\n",
    "and we can recall our previous analysis of Laplace transforms of exponentials to recognize that, for $t \\ge 0$,\n",
    "$$\n",
    "y(t) = P e^{-i\\lambda t} + Q e^{i\\lambda t} = \\frac{e^{i\\lambda t}+e^{-i\\lambda t}}{2} y(0) + \\frac{e^{i\\lambda t}-e^{-i\\lambda t}}{2i}\\frac{y'(0)}{\\lambda} = \\cos(\\lambda t) y(0) + \\sin(\\lambda t)\\frac{y'(0)}{\\lambda},\n",
    "$$\n",
    "which should now be a famlliar result.\n",
    "\n",
    "##### Example: Solving the mass-spring system with zero initial conditions and a unit step force\n",
    "We will now consider the Initial Value Problem\n",
    "$$\n",
    "y''(t) + \\lambda^2 y(t) = u(t),\\;\\;y(0)=y'(0)=0,\n",
    "$$\n",
    "where $u(t)$ is the **unit step function**, which is defined to be equal to zero before $t=0$, and equal to 1 for all $t \\ge 0$. We have already computed the Laplace transforms of $y$ and $y''$, and we can easily find that\n",
    "$$\n",
    "\\mathcal{L}[u](s) = \\int_0^\\infty e^{-st} u(t) dt = \\int_0^\\infty e^{-st} dt = \\left[\\frac{-e^{-st}}{s}\\right]_0^\\infty = \\frac{1}{s},\n",
    "$$\n",
    "if we assume that $s > 0$. Then\n",
    "$$\n",
    "\\mathcal{L}[y](s) = \\frac{1}{s(s^2+\\lambda^2)} = \\frac{1}{s(s+i\\lambda)(s-i\\lambda)} = \\frac{P}{s} + \\frac{Q}{s+i\\lambda} + \\frac{R}{s-i\\lambda}\n",
    "$$\n",
    "for some constants $P$, $Q$, and $R$. Putting the right expression under a common denominator and equating coefficients on $s^0$, $s^1$, and $s^2$ then reveals (please verify this on your own!) that\n",
    "$$\n",
    "P=\\frac{1}{\\lambda^2},\\;\\; Q=R=\\frac{-1}{2\\lambda^2}.\n",
    "$$\n",
    "Thus,\n",
    "$$\n",
    "y(t) = \\frac{1}{\\lambda^2} u(t) - \\frac{1}{2\\lambda^2} e^{-i\\lambda t} u(t) - \\frac{1}{2 \\lambda^2} e^{i\\lambda t} u(t),\n",
    "$$\n",
    "which can be simplified (e.g., via Euler's formula) to\n",
    "$$\n",
    "y(t) = \\frac{1}{\\lambda^2}\\left[ 1 - \\cos(\\lambda t)\\right] u(t).\n",
    "$$\n",
    "This result is easily recognizable as a combination of the steady-state solution $\\frac{1}{\\lambda^2}$ and the unforced solution $\\cos(\\lambda t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework 6 [Due Tuesday, Nov. 17]\n",
    "\n",
    "##### Problem 1\n",
    "Suppose that\n",
    "$$\n",
    "A = X \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} X^{-1},\n",
    "$$\n",
    "with\n",
    "$$\n",
    "X = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Compute $A$, $A^2$, $\\cos(t A)$, $\\sin(t A)$, and $\\exp(t A)$.\n",
    "\n",
    "##### Problem 2\n",
    "Compute the general form of the solution to the differential equation\n",
    "$$\n",
    "y''(x) - x^2 y(x) = 0\n",
    "$$\n",
    "using power series.\n",
    "\n",
    "##### Problem 3\n",
    "Solve the mass-spring system with nonzero initial conditions forced by the unit step funtion,\n",
    "$$\n",
    "  y''(t) + \\lambda^2 y(t) = u(t),\\;\\; y(0)=y_0,\\; y'(0)=y'_0,\n",
    "$$\n",
    "using Laplace transforms.\n",
    "\n",
    "##### Problem 4\n",
    "Solve the mass-spring system with zero initial conditions forced by the unit ramp function, $r(t) = t u(t)$,\n",
    "$$\n",
    "  y''(t) + \\lambda^2 y(t) = r(t),\\;\\; y(0)=y'(0)=0,\n",
    "$$\n",
    "using Laplace transforms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
